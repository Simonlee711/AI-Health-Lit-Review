# AI-Health-Lit-Review
Ai in Healthcare Lit Review 

## Table of Contents 
* [EHR Foundation Models](#section-31-transformer-based-ehr-foundation-models)
* [BERT-Based representation learning of clinical and scientific data](#section-311-bert-based-representation-learning-of-clinical-and-scientific-data)


# EHR Foundation Models

## Transformer-Based EHR Foundation Models
### BERT-Based Representation Learning of Clinical and Scientific Data
| Model              | Source                                                                                     | Link |
|--------------------|--------------------------------------------------------------------------------------------|------|
| BioBERT            | *Bioinformatics*                                                                          | [BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining](https://doi.org/10.1093/bioinformatics/btz682) |
| ClinicalBERT       | *arXiv*                                                                                   | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) |
| BioClinicalBERT    | *arXiv*                                                                                   | [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) |
| MedBERT (Version 1)| *APSIPA ASC*                                                                              | [MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition](http://www.apsipa.org/proceedings/2022/APSIPA%202022/ThAM1-4/1570839765.pdf) |
| MedBERT (Version 2)| *NPJ Digital Medicine*                                                                    | [Med-BERT: Pretrained Contextualized Embeddings on Large-Scale Structured Electronic Health Records for Disease Prediction](https://www.nature.com/articles/s41746-021-00455-y) |
| RadBERT            | *Radiology: Artificial Intelligence*                                                      | [RadBERT: Adapting Transformer-based Language Models to Radiology](https://pubs.rsna.org/doi/full/10.1148/ryai.210258) |
| CEHR-BERT          | *Machine Learning for Health*                                                             | [CEHR-BERT: Incorporating Temporal Information from Structured EHR Data to Improve Prediction Tasks](https://proceedings.mlr.press/v158/) <br> *Note: Must look up paper on site to get PDF.* |
| OMOP Format        | *Applied Clinical Informatics*                                                            | [EHR-Independent Predictive Decision Support Architecture Based on OMOP](https://pubmed.ncbi.nlm.nih.gov/32492716/) |
| BioMedRoBERTa      | *Proceedings of ACL*                                                                      | [Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://aclanthology.org/2020.acl-main.740/) |
| RoBERTa            | *arXiv*                                                                                   | [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) |
| PubMedBERT         | *ACM Transactions on Computing for Healthcare*                                            | [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754) |
| SciBERT            | *arXiv*                                                                                   | [A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676) |



  


