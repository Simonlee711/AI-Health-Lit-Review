# AI-Health-Lit-Review
Ai in Healthcare Lit Review 

## Table of Contents 
* [Section 3.1 Transformer-Based EHR Foundation Models](#section-3-1-transformer-based-ehr-foundation-models)
* [Section 3.1.1 BERT-Based representation learning of clinical and scientific data](#section-3-1-1-bert-based-representation-learning-of-clinical-and-scientific-data)




## Section 3.1 Transformer Based EHR Foundation Models
* BERT: *arXiv* - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

### Section 3.1.1 BERT Based representation learning of clinical and scientific data
* BioBert: *Bioinformatics* - [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://doi.org/10.1093/bioinformatics/btz682)
* Pretraining on PubMed Abstracts: *The NCBI Handbook* - [Pubmed: the bibliographic database](https://www.ncbi.nlm.nih.gov/books/NBK153385/)
* Pretraining on PubMed Articles: *NCBI Handbook* - ?
* ClinicalBERT: *arXiv* - [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342)
* BioClinicalBERT: *arXiv* - [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323)
* MedBERT (Version 1): *Asia-Pacific
Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)* - [MedBERT: A Pre-trained Language Model for
Biomedical Named Entity Recognition](http://www.apsipa.org/proceedings/2022/APSIPA%202022/ThAM1-4/1570839765.pdf)
* MedBERT (Version 2): *NPJ digital medicine* - [Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction](https://www.nature.com/articles/s41746-021-00455-y)
* RadBERT: *Radiology: Artificial Intelligence* - [RadBERT: Adapting Transformer-based Language Models to Radiology](https://pubs.rsna.org/doi/full/10.1148/ryai.210258)
* CEHR-BERT: *Machine Learning for Health* - [CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks](https://proceedings.mlr.press/v158/) *Note must look up paper on site to get pdf of paper*
* OMOP Format: *Applied clinical informatics* - [EHR-Independent Predictive Decision Support Architecture Based on OMOP](https://pubmed.ncbi.nlm.nih.gov/32492716/)
* BioMedRoBERTa: *Proceedings of ACL* - [Donâ€™t stop pretraining: Adapt language models to domains and tasks](https://aclanthology.org/2020.acl-main.740/)
* RoBERTa: *arXiv* -[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
* PubMedBERT: *ACM Transactions on Computing for Healthcare (HEALTH)* - [Domain-specific language model pretraining for biomedical natural language processing](https://dl.acm.org/doi/10.1145/3458754)
* SciBERT: *arXiv* - [A pretrained language model for scientific text](https://arxiv.org/abs/1903.10676)

  


