# A Review of AI in Healthcare in the Era of Foundation Models
---

# EHR Foundation Models

<details>
<summary> Encoder Foundation Models </summary>
  
### BERT-Based Representation Learning of Clinical and Scientific Data
| Model              | Source                                                                                     | Link |
|--------------------|--------------------------------------------------------------------------------------------|------|
| BioBERT            | *Bioinformatics*                                                                          | [BioBERT: A Pre-trained Biomedical Language Representation Model for Biomedical Text Mining](https://doi.org/10.1093/bioinformatics/btz682) |
| ClinicalBERT       | *arXiv*                                                                                   | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) |
| BioClinicalBERT    | *arXiv*                                                                                   | [Publicly Available Clinical BERT Embeddings](https://arxiv.org/abs/1904.03323) |
| MedBERT (Version 1)| *APSIPA ASC*                                                                              | [MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition](http://www.apsipa.org/proceedings/2022/APSIPA%202022/ThAM1-4/1570839765.pdf) |
| MedBERT (Version 2)| *NPJ Digital Medicine*                                                                    | [Med-BERT: Pretrained Contextualized Embeddings on Large-Scale Structured Electronic Health Records for Disease Prediction](https://www.nature.com/articles/s41746-021-00455-y) |
| RadBERT            | *Radiology: Artificial Intelligence*                                                      | [RadBERT: Adapting Transformer-based Language Models to Radiology](https://pubs.rsna.org/doi/full/10.1148/ryai.210258) |
| CEHR-BERT          | *Machine Learning for Health*                                                             | [CEHR-BERT: Incorporating Temporal Information from Structured EHR Data to Improve Prediction Tasks](https://proceedings.mlr.press/v158/) <br> *Note: Must look up paper on site to get PDF.* |
| BioMedRoBERTa      | *Proceedings of ACL*                                                                      | [Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://aclanthology.org/2020.acl-main.740/) |
| RoBERTa            | *arXiv*                                                                                   | [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) |
| PubMedBERT         | *ACM Transactions on Computing for Healthcare*                                            | [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754) |
| SciBERT            | *arXiv*                                                                                   | [A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676) |

### BEHRT-Based temporal modeling
| Model              | Source                                                                                     | Link |
|--------------------|--------------------------------------------------------------------------------------------|------|
| BEHRT              | *Scientific reports*                                                                       | [BEHRT: Transformer for Electronic Health Records](https://www.nature.com/articles/s41598-020-62922-y) |
|CORE-BEHRT          | *arXiv*                                                                                    | [CORE-BEHRT A Carefully Optimized and Rigorously Evaluated BEHRT](https://arxiv.org/html/2404.15201v2) |
|Multimodal BEHRT    | *medRxiv*                                                                                  | [Multimodal BEHRT: Transformers for Multimodal Electronic Health Records to predict breast cancer prognosis](https://www.medrxiv.org/content/10.1101/2024.09.18.24312984v1) |
| Hi-BEHRT           | *IEEE journal of biomedical and health informatics*                                        | [Hi-BEHRT: Hierarchical Transformer-Based Model for Accurate Prediction of Clinical Events Using Multimodal Longitudinal Electronic Health Records](https://pubmed.ncbi.nlm.nih.gov/36427286/)|
| Targeted BEHRT     | *IEEE Transactions on Neural Networks and Learning Systems*                                | [Targeted-BEHRT: Deep Learning for Observational Causal Inference on Longitudinal Electronic Health Records](https://pubmed.ncbi.nlm.nih.gov/35737602/) |
| ExBEHRT            | *International Workshop on Trustworthy Machine Learning for Healthcare*                   | [ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions](https://arxiv.org/abs/2303.12364) |
| MEME               | *arXiv*                                                                                    | [Multimodal Clinical Pseudo-notes for Emergency Department Prediction Tasks using Multiple Embedding Model for EHR (MEME)](https://arxiv.org/html/2402.00160v1)

### Other Transformer-Based Models
| Model              | Source                                                                                     | Link |
|--------------------|--------------------------------------------------------------------------------------------|------|
| MiME               | *Advances in neural information processing systems*                                        | [MiME: Multilevel Medical Embedding of Electronic Health Records for Predictive Healthcare](https://papers.nips.cc/paper_files/paper/2018/hash/934b535800b1cba8f96a5d72f72f1611-Abstract.html)| 
| BioMegatron        | *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)* | [BioMegatron: Larger Biomedical Domain Language Model](https://aclanthology.org/2020.emnlp-main.379/)

</details>

<details>
<summary> EHR Autoregressive Foundation Models </summary>

# Autoregressive EHR Foundation Models
### Early Autoregressive Models
| Model              | Source                                                                                     | Link |
|--------------------|--------------------------------------------------------------------------------------------|------|
| Doctor AI          | *Machine learning for healthcare conference*                                               | [Doctor AI: Predicting Clinical Events via Recurrent Neural Networks](https://pubmed.ncbi.nlm.nih.gov/28286600/) |

</details>

<details>
<summary> EHR LLMs </summary>
Fill in
</details>

# Clinical Foundation models

## Medical Imaging Foundation Modes

<details>
<summary> SAM Based Models </summary>
[fill in]
</details>

<details>
<summary> Other medical Imaging Foundation Models </summary>
[fill in]
</details>

<details>
<summary> Frameworks </summary>
[fill in]
</details>

## Genetic & Genomics Foundation Modes

<details>
<summary> Genetic Foundation Models </summary>
[fill in]
</details>

<details>
<summary> Long Sequence Modeling </summary>
[fill in]
</details>

<details>
<summary> Special Eigenetic Foundation models </summary>
[fill in]
</details>

## Physiological Signals and Waveforms Foundation Modes

<details>
<summary> ECG Foundation Models </summary>
[fill in]
</details>

<details>
<summary> EEG Foundation models </summary>
[fill in]
</details>

<details>
<summary> Multimodal Foundation models </summary>
[fill in]
</details>

<details>
<summary> Other Foundation models </summary>
[fill in]
</details>

### Contact

- Simon Lee (simonlee711@g.ucla.edu)
- Ava Gonick


  


